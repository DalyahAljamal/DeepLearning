
# Fix for juliabox
if ENV["HOME"] == "/mnt/juliabox"; Pkg.dir(path...)=joinpath("/home/jrun/.julia/v0.6",path...); end
using Knet
gpu()

Pkg.add("Plots")
using Plots
  
using Compat,GZip # Helpers to read the MNIST (Like lab-2)


# Dolwnloading data ICVL Dataset
# Reading Training data




# Generate Random Data and Minibatching
# Input img:128*128
#Batch Size= 128

xtrn= 0.1*rand(128,128,1,5000);
ytrn= 0.1*rand(48, 5000);

xtst= 0.1*rand(128,128,1,1000);
ytst= 0.1*rand(48, 1000);

Atype = gpu() >= 0 ? KnetArray{Float32} : Array{Float32}
dtst = minibatch(xtst,ytst,120;xtype=Atype); 
dtrn = minibatch(xtrn,ytrn,120; xtype=Atype); 


#print(size(xtrn))
#print(size(ytrn))
#print(size(xtst))
#print(size(ytst))
#print(size(dtst))
#print(dtrn[1])

#Loss Function
#Mean Loss
loss(w,x,y)= mean(abs2,y-predict(w,x))

#loss(w,x,ygold) =nll(predict(w,x),ygold)


#Gradient Loss
lossgradient = grad(loss);

# CNN Predict Function

#Conv 8(5*5) , Max Pool (4*4), Conv 8(5*5), Max Pool (2*2), Conv 8(3*3), 2 FC (1024),droupout, PCA (30), FC(48)

win=[4,0,2]
function predict(w,x) 
    n=length(w)-8
    for i=1:2:n # we are jumping 2 steps since alwase we have w then b, then w  then b....
        x = pool(relu.(conv4(w[i],x) .+ w[i+1]); window=win[i]) 
    end
    i=n+1
    x= relu.(conv4(w[i],x) .+ w[i+1])
    for i=n+3:2:length(w)-2 # Fully connected layers
        x = relu.(w[i]*mat(x) .+ w[i+1])
    end
    # Add PCA layer later
    #print(size( w[end-1]*mat(x) .+ w[end]  ))
    return w[end-1]*mat(x) .+ w[end]  
end

#Parameter Intialization

wcnn=map(Atype, [ 0.1*randn(5,5,1,8),  zeros(1,1,8,1), 
        0.1*randn(5,5,8,8),  zeros(1,1,8,1),
        0.1*randn(3,3,8,8),  zeros(1,1,8,1), #103968
        0.1*randn(1024,968),  zeros(1024,1),
        0.1*randn(1024,1024),  zeros(1024,1), # add PCA Layer parameters later
        0.1*randn(48,1024), zeros(48,1)])

#TRAIN CNN Function

function train!(w, data; lr=.01)
    for (x,y) in data
        print(size(x))
        print(size(y))
        
        print(loss(w,x,y))
        dw = lossgradient(w, x, y)
        for i in 1:length(w) # Loop  over W and B
            w[i] -= lr * dw[i]
        end
        print(w[i])
    end
    
    return w
end

#CNN Loss
#@time trncnnloss = [mean(abs2,ytrn-predict(w,xtrn)) for w in cnnmodels ]; 
#@time tstcnnloss = [ mean(abs2,ytst-predict(w,xtst)) for w in cnnmodels ]; 

trncnnloss=0
tstcnnloss =0
c=1

for (x,y) in dtrn
    trncnnloss =+ mean(abs2,y-predict(wcnn,x))
    c+=1
end
for (x,y) in dtst
    tstcnnloss = + mean(abs2,y-predict(wcnn,x)) 
end

print(trncnnloss/c)
print(tstcnnloss/c)

##Accuracy

#trnAcc= accuracy(wcnn,dtrn,predict)
#tstAcc=accuracy(wcnn,dtst,predict)

trnAcc=0
tstAcc =0
c=1

for (x,y) in dtrn
    trnAcc =+ accuracy(predict(wcnn,x),y)
    c+=1
end
for (x,y) in dtst
    tstAcc = + accuracy(predict(wcnn,x),y) 
end

print(trnAcc/c)
print(tstAcc/c)

#MAIN

@time cnnmodels = [ copy(train!(wcnn, dtrn)) for epoch=1:100 ];

